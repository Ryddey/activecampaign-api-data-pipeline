{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc966d0",
   "metadata": {},
   "source": [
    "# ActiveCampaign — Full Chatter (v3.2) — Notebook autocontenido\n",
    "\n",
    "Este notebook contiene **todo el código** del pipeline (sin imports externos), más una celda final para ejecutarlo.\n",
    "\n",
    "## Salidas clave\n",
    "- `master/mart/chatter_master.csv`  → timeline unificado y **legible** por contacto (IDs traducidos a nombres)\n",
    "- `master/mart/contact_digest.csv`  → “mensaje claro” por contacto\n",
    "- `dims/`                           → dimensiones cacheadas (campaigns, messages, automations, tags, lists, users, etc.)\n",
    "- `master/raw/` y `master/latest/`  → histórico y snapshot\n",
    "\n",
    "> Requisitos: `pandas`, `requests`, `tqdm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175bf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ActiveCampaign — Extracción HISTÓRICA + INCREMENTAL + DEDUPE (sin perder info) + FULL CHATTER + AUTOMATIONS\n",
    "\n",
    "Qué agrega esta versión:\n",
    "- /contacts/{id}/automationEntryCounts (cuántas veces entró a cada automatización) \n",
    "- /contacts/{id}/contactAutomations   (detalle de automatizaciones activas/completadas por contacto) \n",
    "- Enriquecimiento de emailActivities con:\n",
    " - campaign_name + message_subject + automation_name (usando dimensiones /campaigns, /messages, /automations) \n",
    "\n",
    "Notas importantes:\n",
    "- \"automationEntryCounts\" NO trae \"mensajes enviados\". Solo trae automatización + estado + contador. \n",
    "- Para ver qué email/campaña se envió y qué abrió/clickeó el contacto, usamos /emailActivities y lo enlazamos con /campaigns y /messages. \n",
    "- Paginación/filters: limit/offset y filters[...] \n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class ACConfig:\n",
    "  base_url: str         # e.g. https://YOURACCOUNT.api-us1.com/api/3\n",
    "  api_token: str\n",
    "  page_limit: int = 100\n",
    "  rate_sleep: float = 0.25   # 0.25s ≈ 4 req/s (seguro bajo límite 5 req/s)\n",
    "  total_retries: int = 6\n",
    "  backoff_factor: float = 0.5\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utils I/O\n",
    "# -------------------------\n",
    "def ensure_dir(path: str) -> None:\n",
    "  os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def utc_now_iso() -> str:\n",
    "  return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "\n",
    "def safe_int(x: Any, default: int = 0) -> int:\n",
    "  try:\n",
    "    return int(str(x).strip())\n",
    "  except Exception:\n",
    "    return default\n",
    "\n",
    "\n",
    "def write_csv_utf8sig(path: str, df: pd.DataFrame) -> None:\n",
    "  ensure_dir(os.path.dirname(path))\n",
    "  df.to_csv(path, index=False, encoding=\"utf-8-sig\", quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "\n",
    "def load_json(path: str, default: Any) -> Any:\n",
    "  if not os.path.exists(path):\n",
    "    return default\n",
    "  with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    return json.load(f)\n",
    "\n",
    "\n",
    "def save_json(path: str, obj: Any) -> None:\n",
    "  ensure_dir(os.path.dirname(path))\n",
    "  with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def extract_id_from_url(url: str) -> str:\n",
    "  if not url:\n",
    "    return \"\"\n",
    "  m = re.search(r\"/(\\d+)(?:\\?|$)\", str(url))\n",
    "  return m.group(1) if m else \"\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Dedupe persistence (delta + master/raw + master/latest)\n",
    "# -------------------------\n",
    "def _normalize_for_hash(v: Any) -> str:\n",
    "  \"\"\"Normaliza valores (incluye listas/dicts/arrays) para hash estable.\"\"\"\n",
    "  if v is None:\n",
    "    return \"\"\n",
    "  # NA / NaN (pd.isna puede devolver array para listas, por eso try/except)\n",
    "  try:\n",
    "    if pd.isna(v):\n",
    "      return \"\"\n",
    "  except Exception:\n",
    "    pass\n",
    "  # Timestamps\n",
    "  if isinstance(v, (pd.Timestamp, datetime)):\n",
    "    return v.isoformat()\n",
    "  # bytes\n",
    "  if isinstance(v, (bytes, bytearray)):\n",
    "    try:\n",
    "      return v.decode('utf-8', errors='replace')\n",
    "    except Exception:\n",
    "      return str(v)\n",
    "  # numpy arrays u objetos similares\n",
    "  if hasattr(v, 'tolist') and not isinstance(v, (str, bytes, bytearray)):\n",
    "    try:\n",
    "      v = v.tolist()\n",
    "    except Exception:\n",
    "      pass\n",
    "  # colecciones -> JSON estable\n",
    "  if isinstance(v, (list, dict, tuple, set)):\n",
    "    if isinstance(v, (tuple, set)):\n",
    "      v = list(v)\n",
    "    return json.dumps(v, ensure_ascii=False, sort_keys=True, default=str)\n",
    "  return str(v)\n",
    "\n",
    "\n",
    "def df_rowhash(df: pd.DataFrame, cols: List[str]) -> pd.Series:\n",
    "  # Hash estable por contenido (sin reventar con listas/dicts/arrays)\n",
    "  def _h(row: pd.Series) -> str:\n",
    "    s = \"||\".join(_normalize_for_hash(row.get(c)) for c in cols)\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "  return df.apply(_h, axis=1)\n",
    "\n",
    "\n",
    "def persist_table_dual(\n",
    "  *,\n",
    "  output_base: str,\n",
    "  run_id: str,\n",
    "  table_name: str,\n",
    "  df_new: pd.DataFrame,\n",
    "  key_cols_latest: Optional[List[str]] = None,\n",
    "  updated_at_col: Optional[str] = None,\n",
    ") -> None:\n",
    "  \"\"\"\n",
    "  - runs/<run_id>/delta/<table>.csv : delta de esta ejecución\n",
    "  - master/raw/<table>.csv     : acumulado dedupe por hash\n",
    "  - master/latest/<table>.csv   : snapshot por key_cols_latest (último por updated_at_col o extracted_at)\n",
    "  \"\"\"\n",
    "  if df_new is None or df_new.empty:\n",
    "    return\n",
    "\n",
    "  df_new = df_new.copy()\n",
    "  df_new[\"run_id\"] = run_id\n",
    "  df_new[\"extracted_at\"] = utc_now_iso()\n",
    "\n",
    "  # 1) Delta\n",
    "  delta_path = os.path.join(output_base, \"runs\", run_id, \"delta\", f\"{table_name}.csv\")\n",
    "  write_csv_utf8sig(delta_path, df_new)\n",
    "\n",
    "  # 2) Master RAW (dedupe por hash)\n",
    "  raw_dir = os.path.join(output_base, \"master\", \"raw\")\n",
    "  ensure_dir(raw_dir)\n",
    "  raw_path = os.path.join(raw_dir, f\"{table_name}.csv\")\n",
    "\n",
    "  if os.path.exists(raw_path):\n",
    "    df_old = pd.read_csv(raw_path, dtype=str)\n",
    "    df_all = pd.concat([df_old, df_new], ignore_index=True)\n",
    "  else:\n",
    "    df_all = df_new\n",
    "\n",
    "  content_cols = [c for c in df_all.columns if c not in (\"run_id\", \"extracted_at\", \"_row_hash\")]\n",
    "  df_all[\"_row_hash\"] = df_rowhash(df_all, content_cols)\n",
    "  df_all = df_all.drop_duplicates(subset=[\"_row_hash\"], keep=\"first\")\n",
    "  write_csv_utf8sig(raw_path, df_all)\n",
    "\n",
    "  # 3) Master LATEST (snapshot por key)\n",
    "  if key_cols_latest:\n",
    "    latest_dir = os.path.join(output_base, \"master\", \"latest\")\n",
    "    ensure_dir(latest_dir)\n",
    "    latest_path = os.path.join(latest_dir, f\"{table_name}.csv\")\n",
    "\n",
    "    # Partimos del raw dedupe (no del delta) para evitar inconsistencias\n",
    "    df_latest_src = df_all.copy()\n",
    "\n",
    "    sort_col = updated_at_col if updated_at_col and updated_at_col in df_latest_src.columns else \"extracted_at\"\n",
    "    # orden asc y nos quedamos con el último por key\n",
    "    df_latest_src = df_latest_src.sort_values(by=sort_col, ascending=True, kind=\"mergesort\")\n",
    "    df_latest = df_latest_src.drop_duplicates(subset=key_cols_latest, keep=\"last\")\n",
    "    write_csv_utf8sig(latest_path, df_latest)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# ActiveCampaign client\n",
    "# -------------------------\n",
    "class ACClient:\n",
    "  def __init__(self, cfg: ACConfig):\n",
    "    self.cfg = cfg\n",
    "    self.session = requests.Session()\n",
    "    self.session.headers.update({\n",
    "      \"accept\": \"application/json\",\n",
    "      \"Api-Token\": cfg.api_token,\n",
    "    })\n",
    "\n",
    "  def _request(self, method: str, path: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    url = self.cfg.base_url.rstrip(\"/\") + \"/\" + path.lstrip(\"/\")\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(self.cfg.total_retries + 1):\n",
    "      try:\n",
    "        r = self.session.request(method, url, params=params, timeout=60)\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "          raise requests.HTTPError(f\"HTTP {r.status_code}: {r.text[:200]}\", response=r)\n",
    "        r.raise_for_status()\n",
    "        return r.json() if r.text else {}\n",
    "      except Exception as e:\n",
    "        last_err = e\n",
    "        sleep_s = (self.cfg.backoff_factor * (2 ** attempt))\n",
    "        time.sleep(min(30.0, sleep_s))\n",
    "    raise last_err # type: ignore\n",
    "\n",
    "  def get(self, path: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    return self._request(\"GET\", path, params=params)\n",
    "\n",
    "  def get_paginated_offset(\n",
    "    self,\n",
    "    collection_key: str,\n",
    "    path: str,\n",
    "    base_params: Optional[Dict[str, Any]] = None,\n",
    "    limit: Optional[int] = None,\n",
    "  ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Paginación offset/limit estándar. \"\"\"\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    offset = 0\n",
    "    page_limit = int(limit or self.cfg.page_limit)\n",
    "    while True:\n",
    "      params = dict(base_params or {})\n",
    "      params.update({\"limit\": page_limit, \"offset\": offset})\n",
    "      data = self.get(path, params=params)\n",
    "      items = data.get(collection_key, [])\n",
    "      if isinstance(items, dict):\n",
    "        items = [items]\n",
    "      if not items:\n",
    "        break\n",
    "      out.extend(items)\n",
    "      if len(items) < page_limit:\n",
    "        break\n",
    "      offset += page_limit\n",
    "      time.sleep(self.cfg.rate_sleep)\n",
    "    return out\n",
    "\n",
    "  def get_contacts_after_id(self, last_id: int, limit: int = 100) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Trae contactos nuevos con id_greater + orders[id]=ASC (mejor rendimiento para cuentas grandes). \n",
    "    \"\"\"\n",
    "    all_new: List[Dict[str, Any]] = []\n",
    "    cursor = int(last_id or 0)\n",
    "    while True:\n",
    "      data = self.get(\n",
    "        \"/contacts\",\n",
    "        params={\"limit\": limit, \"orders[id]\": \"ASC\", \"id_greater\": cursor},\n",
    "      )\n",
    "      batch = data.get(\"contacts\", [])\n",
    "      if isinstance(batch, dict):\n",
    "        batch = [batch]\n",
    "      if not batch:\n",
    "        break\n",
    "      all_new.extend(batch)\n",
    "      cursor = max(cursor, max((safe_int(c.get(\"id\")) for c in batch), default=cursor))\n",
    "      if len(batch) < limit:\n",
    "        break\n",
    "      time.sleep(self.cfg.rate_sleep)\n",
    "    return all_new\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Seed + contact universe\n",
    "# -------------------------\n",
    "def resolve_seed_contact_ids(\n",
    "  client: ACClient,\n",
    "  seed_csv_path: str,\n",
    "  *,\n",
    "  id_col_candidates: List[str] = [\"id\", \"contact_id\", \"contactid\", \"ID\"],\n",
    "  email_col_candidates: List[str] = [\"email\", \"Email\", \"EMAIL\"],\n",
    "  use_email_fallback: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Lee CSV semilla y resuelve contact_id.\n",
    "  - Si hay columna id/contact_id -> usa eso.\n",
    "  - Si no, y hay email -> busca /contacts?filters[email]=... (puede ser más lento).\n",
    "  \"\"\"\n",
    "  df = pd.read_csv(seed_csv_path, dtype=str).fillna(\"\")\n",
    "  cols = list(df.columns)\n",
    "\n",
    "  def pick_col(cands: List[str]) -> str:\n",
    "    for c in cands:\n",
    "      if c in cols:\n",
    "        return c\n",
    "    # intenta case-insensitive\n",
    "    lower_map = {c.lower(): c for c in cols}\n",
    "    for c in cands:\n",
    "      if c.lower() in lower_map:\n",
    "        return lower_map[c.lower()]\n",
    "    return \"\"\n",
    "\n",
    "  id_col = pick_col(id_col_candidates)\n",
    "  email_col = pick_col(email_col_candidates)\n",
    "\n",
    "  out_rows: List[Dict[str, str]] = []\n",
    "\n",
    "  if id_col:\n",
    "    for _, r in df.iterrows():\n",
    "      cid = str(r.get(id_col, \"\")).strip()\n",
    "      if cid:\n",
    "        out_rows.append({\"contact_id\": cid, \"email\": str(r.get(email_col, \"\")).strip() if email_col else \"\"})\n",
    "    return pd.DataFrame(out_rows).drop_duplicates(subset=[\"contact_id\"])\n",
    "\n",
    "  if use_email_fallback and email_col:\n",
    "    for _, r in tqdm(df.iterrows(), total=len(df), desc=\"Resolve seed emails\"):\n",
    "      email = str(r.get(email_col, \"\")).strip()\n",
    "      if not email:\n",
    "        continue\n",
    "      data = client.get(\"/contacts\", params={\"filters[email]\": email, \"limit\": 1})\n",
    "      contacts = data.get(\"contacts\", [])\n",
    "      if isinstance(contacts, dict):\n",
    "        contacts = [contacts]\n",
    "      if contacts:\n",
    "        out_rows.append({\"contact_id\": str(contacts[0].get(\"id\", \"\")).strip(), \"email\": email})\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    return pd.DataFrame(out_rows).drop_duplicates(subset=[\"contact_id\"])\n",
    "\n",
    "  return pd.DataFrame(columns=[\"contact_id\", \"email\"])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Dimensions (cacheable)\n",
    "# -------------------------\n",
    "def load_or_refresh_dim(\n",
    "  *,\n",
    "  client: ACClient,\n",
    "  output_base: str,\n",
    "  dim_name: str,\n",
    "  path: str,\n",
    "  collection_key: str,\n",
    "  refresh_days: int = 7,\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Guarda dims en output_base/dims/<dim_name>.csv.\n",
    "  Refresca si:\n",
    "   - no existe, o\n",
    "   - archivo más viejo que refresh_days\n",
    "  \"\"\"\n",
    "  dims_dir = os.path.join(output_base, \"dims\")\n",
    "  ensure_dir(dims_dir)\n",
    "  csv_path = os.path.join(dims_dir, f\"{dim_name}.csv\")\n",
    "  meta_path = os.path.join(dims_dir, f\"{dim_name}.meta.json\")\n",
    "\n",
    "  meta = load_json(meta_path, {})\n",
    "  last = meta.get(\"fetched_at_utc\")\n",
    "  must_refresh = not os.path.exists(csv_path)\n",
    "  if last and not must_refresh:\n",
    "    try:\n",
    "      dt = datetime.strptime(last, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
    "      age_days = (datetime.now(timezone.utc) - dt).days\n",
    "      must_refresh = age_days >= refresh_days\n",
    "    except Exception:\n",
    "      must_refresh = True\n",
    "\n",
    "  if must_refresh:\n",
    "    try:\n",
    "      items = client.get_paginated_offset(collection_key, path)\n",
    "      df = pd.DataFrame(items)\n",
    "      write_csv_utf8sig(csv_path, df)\n",
    "      save_json(meta_path, {\"fetched_at_utc\": utc_now_iso(), \"path\": path, \"collection_key\": collection_key})\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "      return df\n",
    "    except requests.HTTPError as e:\n",
    "      code = getattr(getattr(e, 'response', None), 'status_code', None)\n",
    "      # Algunas cuentas no tienen ciertos recursos (p.ej. /scores, /accounts). No rompemos el pipeline.\n",
    "      if code == 404:\n",
    "        df = pd.DataFrame()\n",
    "        write_csv_utf8sig(csv_path, df)\n",
    "        save_json(meta_path, {\"fetched_at_utc\": utc_now_iso(), \"path\": path, \"collection_key\": collection_key, \"error\": f\"HTTP {code}\"})\n",
    "        return df\n",
    "      raise\n",
    "\n",
    "  return pd.read_csv(csv_path, dtype=str)\n",
    "\n",
    "\n",
    "def load_all_dims(client: ACClient, output_base: str, refresh_days: int = 7) -> Dict[str, pd.DataFrame]:\n",
    "  dims = {}\n",
    "  dims[\"campaigns\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"campaigns\", path=\"/campaigns\", collection_key=\"campaigns\", refresh_days=refresh_days) # \n",
    "  dims[\"messages\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"messages\", path=\"/messages\", collection_key=\"messages\", refresh_days=refresh_days)  # \n",
    "  dims[\"automations\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"automations\", path=\"/automations\", collection_key=\"automations\", refresh_days=refresh_days) # \n",
    "  dims[\"users\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"users\", path=\"/users\", collection_key=\"users\", refresh_days=refresh_days)\n",
    "  dims[\"lists\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"lists\", path=\"/lists\", collection_key=\"lists\", refresh_days=refresh_days)\n",
    "  dims[\"tags\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"tags\", path=\"/tags\", collection_key=\"tags\", refresh_days=refresh_days)\n",
    "  dims[\"fields\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"fields\", path=\"/fields\", collection_key=\"fields\", refresh_days=refresh_days)\n",
    "  # CRM extras (para traducir ids a nombres)\n",
    "  # Pipelines/Stages (deals)\n",
    "  dims[\"dealGroups\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"dealGroups\", path=\"/dealGroups\", collection_key=\"dealGroups\", refresh_days=refresh_days)\n",
    "  dims[\"dealStages\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"dealStages\", path=\"/dealStages\", collection_key=\"dealStages\", refresh_days=refresh_days)\n",
    "  # Accounts + Scores (si tu cuenta los usa)\n",
    "  dims[\"accounts\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"accounts\", path=\"/accounts\", collection_key=\"accounts\", refresh_days=refresh_days)\n",
    "  dims[\"scores\"] = load_or_refresh_dim(client=client, output_base=output_base, dim_name=\"scores\", path=\"/scores\", collection_key=\"scores\", refresh_days=refresh_days)\n",
    "  return dims\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Contact chatter extractors\n",
    "# -------------------------\n",
    "STATUS_MAP_AUT = {\"1\": \"Active\", \"0\": \"Inactive\", 1: \"Active\", 0: \"Inactive\"}\n",
    "HIDDEN_MAP = {\"1\": \"Yes\", \"0\": \"No\", 1: \"Yes\", 0: \"No\"}\n",
    "\n",
    "\n",
    "def _safe_list(data: Dict[str, Any], key: str) -> List[Dict[str, Any]]:\n",
    "  items = data.get(key, [])\n",
    "  if isinstance(items, dict):\n",
    "    items = [items]\n",
    "  if not isinstance(items, list):\n",
    "    return []\n",
    "  return [x for x in items if isinstance(x, dict)]\n",
    "\n",
    "\n",
    "def run_activities(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  \"\"\"Activity stream del contacto. /activities?contact={id}\"\"\"\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"Activities\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(\"/activities\", params={\"contact\": cid, \"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"activities\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_email_activities(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Email activities (open/click/etc) filtrado por subscriberid. \n",
    "  \"\"\"\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"EmailActivities\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(\n",
    "        \"/emailActivities\",\n",
    "        params={\"filters[subscriberid]\": cid, \"limit\": client.cfg.page_limit, \"offset\": offset},\n",
    "      )\n",
    "      items = _safe_list(data, \"emailActivities\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_contact_notes(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"ContactNotes\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(f\"/contacts/{cid}/notes\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"notes\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_contact_lists(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"ContactLists\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(f\"/contacts/{cid}/contactLists\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"contactLists\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_contact_tags(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"ContactTags\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(f\"/contacts/{cid}/contactTags\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"contactTags\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_contact_logs(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"ContactLogs\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(f\"/contacts/{cid}/contactLogs\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"contactLogs\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_tracking_logs(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"TrackingLogs\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      try:\n",
    "        data = client.get(f\"/contacts/{cid}/trackingLogs\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      except requests.HTTPError as e:\n",
    "        code = getattr(getattr(e, 'response', None), 'status_code', None)\n",
    "        # 404 suele significar: sin logs / feature no activa / contacto no encontrado\n",
    "        if code == 404:\n",
    "          break\n",
    "        raise\n",
    "      items = _safe_list(data, \"trackingLogs\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_bounce_logs(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"BounceLogs\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      try:\n",
    "        data = client.get(f\"/contacts/{cid}/bounceLogs\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      except requests.HTTPError as e:\n",
    "        code = getattr(getattr(e, 'response', None), 'status_code', None)\n",
    "        # 404 suele significar: sin bounce logs / contacto no tiene bounces / contacto no existe\n",
    "        if code == 404:\n",
    "          break\n",
    "        raise\n",
    "      items = _safe_list(data, \"bounceLogs\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_geo_ips(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"GeoIps\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(f\"/contacts/{cid}/geoIps\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"geoIps\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_contact_goals(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"ContactGoals\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(f\"/contacts/{cid}/contactGoals\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"contactGoals\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_contact_data(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"ContactData\"):\n",
    "    try:\n",
    "      data = client.get(f\"/contacts/{cid}/contactData\")\n",
    "      items = _safe_list(data, \"contactData\")\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "    except requests.HTTPError:\n",
    "      pass\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_score_values(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"ScoreValues\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(f\"/contacts/{cid}/scoreValues\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"scoreValues\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_account_contacts(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"AccountContacts\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(f\"/contacts/{cid}/accountContacts\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"accountContacts\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_contact_tasks(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Tasks asociadas DIRECTAMENTE al contacto (reltype=Subscriber).\n",
    "  \"\"\"\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"ContactTasks\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(\n",
    "        \"/dealTasks\",\n",
    "        params={\n",
    "          \"filters[reltype]\": \"Subscriber\",\n",
    "          \"filters[relid]\": cid,\n",
    "          \"limit\": client.cfg.page_limit,\n",
    "          \"offset\": offset,\n",
    "        },\n",
    "      )\n",
    "      items = _safe_list(data, \"dealTasks\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_contact_automations(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Detalle de automatizaciones donde el contacto está/estuvo (contactAutomations).\n",
    "  (Se ve también en GET /contacts/:id example) \n",
    "  \"\"\"\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"ContactAutomations\"):\n",
    "    offset = 0\n",
    "    while True:\n",
    "      data = client.get(f\"/contacts/{cid}/contactAutomations\", params={\"limit\": client.cfg.page_limit, \"offset\": offset})\n",
    "      items = _safe_list(data, \"contactAutomations\")\n",
    "      if not items:\n",
    "        break\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        rows.append(row)\n",
    "      if len(items) < client.cfg.page_limit:\n",
    "        break\n",
    "      offset += client.cfg.page_limit\n",
    "      time.sleep(client.cfg.rate_sleep)\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_automation_entry_counts(client: ACClient, contact_ids: List[str]) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Cuántas veces entró el contacto a cada automatización. \n",
    "  \"\"\"\n",
    "  rows: List[Dict[str, Any]] = []\n",
    "  for cid in tqdm(contact_ids, desc=\"AutomationEntryCounts\"):\n",
    "    try:\n",
    "      data = client.get(f\"/contacts/{cid}/automationEntryCounts\")\n",
    "      items = _safe_list(data, \"automationEntryCounts\")\n",
    "      for it in items:\n",
    "        row = dict(it)\n",
    "        row[\"contact_id\"] = str(cid)\n",
    "        # normalización amigable\n",
    "        row[\"status_label\"] = STATUS_MAP_AUT.get(it.get(\"status\"), it.get(\"status\"))\n",
    "        row[\"hidden_label\"] = HIDDEN_MAP.get(it.get(\"hidden\"), it.get(\"hidden\"))\n",
    "        rows.append(row)\n",
    "    except requests.HTTPError:\n",
    "      pass\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "  return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Deals bundle (deals + dealTasks + dealNotes + dealActivities)\n",
    "# -------------------------\n",
    "def fetch_deals_for_contact(client: ACClient, contact_id: str) -> List[Dict[str, Any]]:\n",
    "  try:\n",
    "    deals = client.get_paginated_offset(\"deals\", \"/deals\", base_params={\"filters[contact]\": contact_id})\n",
    "    for d in deals:\n",
    "      d[\"contact_id\"] = str(contact_id)\n",
    "    return deals\n",
    "  except requests.HTTPError:\n",
    "    return []\n",
    "\n",
    "\n",
    "def fetch_deal_notes(client: ACClient, deal_id: str, contact_id: str) -> List[Dict[str, Any]]:\n",
    "  try:\n",
    "    notes = client.get_paginated_offset(\"notes\", f\"/deals/{deal_id}/notes\")\n",
    "    for n in notes:\n",
    "      n[\"deal_id\"] = str(deal_id)\n",
    "      n[\"contact_id\"] = str(contact_id)\n",
    "    return notes\n",
    "  except requests.HTTPError:\n",
    "    return []\n",
    "\n",
    "\n",
    "def fetch_deal_tasks(client: ACClient, deal_id: str, contact_id: str) -> List[Dict[str, Any]]:\n",
    "  try:\n",
    "    tasks = client.get_paginated_offset(\"dealTasks\", f\"/deals/{deal_id}/dealTasks\")\n",
    "    for t in tasks:\n",
    "      t[\"deal_id\"] = str(deal_id)\n",
    "      t[\"contact_id\"] = str(contact_id)\n",
    "    return tasks\n",
    "  except requests.HTTPError:\n",
    "    return []\n",
    "\n",
    "\n",
    "def fetch_deal_activities(client: ACClient, deal_id: str, contact_id: str) -> List[Dict[str, Any]]:\n",
    "  try:\n",
    "    acts = client.get_paginated_offset(\"dealActivities\", f\"/deals/{deal_id}/dealActivities\")\n",
    "    for a in acts:\n",
    "      a[\"deal_id\"] = str(deal_id)\n",
    "      a[\"contact_id\"] = str(contact_id)\n",
    "    return acts\n",
    "  except requests.HTTPError:\n",
    "    return []\n",
    "\n",
    "\n",
    "def run_deals_bundle(client: ACClient, contact_ids: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "  deals_rows: List[Dict[str, Any]] = []\n",
    "  notes_rows: List[Dict[str, Any]] = []\n",
    "  tasks_rows: List[Dict[str, Any]] = []\n",
    "  acts_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "  for cid in tqdm(contact_ids, desc=\"Deals\"):\n",
    "    deals = fetch_deals_for_contact(client, cid)\n",
    "    deals_rows.extend(deals)\n",
    "\n",
    "    for d in deals:\n",
    "      did = str(d.get(\"id\", \"\")).strip()\n",
    "      if not did:\n",
    "        continue\n",
    "      notes_rows.extend(fetch_deal_notes(client, did, cid))\n",
    "      tasks_rows.extend(fetch_deal_tasks(client, did, cid))\n",
    "      acts_rows.extend(fetch_deal_activities(client, did, cid))\n",
    "\n",
    "    time.sleep(client.cfg.rate_sleep)\n",
    "\n",
    "  return (\n",
    "    pd.DataFrame(deals_rows),\n",
    "    pd.DataFrame(notes_rows),\n",
    "    pd.DataFrame(tasks_rows),\n",
    "    pd.DataFrame(acts_rows),\n",
    "  )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Enrichment\n",
    "# -------------------------\n",
    "def enrich_email_activities(\n",
    "  df_email: pd.DataFrame,\n",
    "  dims: Dict[str, pd.DataFrame],\n",
    ") -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Enriquecimiento sin perder info: agrega columnas nuevas.\n",
    "  - campaign_name\n",
    "  - message_subject\n",
    "  - automation_id + automation_name (derivado de campaign.links.automation o seriesid)\n",
    "  \"\"\"\n",
    "  if df_email is None or df_email.empty:\n",
    "    return df_email\n",
    "\n",
    "  df = df_email.copy()\n",
    "\n",
    "  campaigns = dims.get(\"campaigns\", pd.DataFrame()).copy()\n",
    "  messages = dims.get(\"messages\", pd.DataFrame()).copy()\n",
    "  automations = dims.get(\"automations\", pd.DataFrame()).copy()\n",
    "\n",
    "  for d in (campaigns, messages, automations):\n",
    "    for col in [\"id\", \"message_id\", \"seriesid\", \"automation\"]:\n",
    "      if col in d.columns:\n",
    "        d[col] = d[col].astype(str)\n",
    "\n",
    "  # campaign_id column candidate\n",
    "  camp_col = None\n",
    "  for c in [\"campaignid\", \"campaignId\", \"campaign\", \"campaign_id\"]:\n",
    "    if c in df.columns:\n",
    "      camp_col = c\n",
    "      break\n",
    "\n",
    "  if camp_col and \"id\" in campaigns.columns:\n",
    "    camp_name_map = dict(zip(campaigns[\"id\"].astype(str), campaigns.get(\"name\", \"\").astype(str)))\n",
    "    df[\"campaign_id_norm\"] = df[camp_col].astype(str)\n",
    "    df[\"campaign_name\"] = df[\"campaign_id_norm\"].map(camp_name_map)\n",
    "\n",
    "    # message_id from event or from campaign\n",
    "    msg_id_from_event = None\n",
    "    for c in [\"messageid\", \"messageId\", \"message\", \"message_id\"]:\n",
    "      if c in df.columns:\n",
    "        msg_id_from_event = c\n",
    "        break\n",
    "\n",
    "    if msg_id_from_event:\n",
    "      df[\"message_id_norm\"] = df[msg_id_from_event].astype(str)\n",
    "    elif \"message_id\" in campaigns.columns:\n",
    "      msg_map = dict(zip(campaigns[\"id\"].astype(str), campaigns[\"message_id\"].astype(str)))\n",
    "      df[\"message_id_norm\"] = df[\"campaign_id_norm\"].map(msg_map)\n",
    "    else:\n",
    "      df[\"message_id_norm\"] = \"\"\n",
    "\n",
    "    # subject\n",
    "    if \"id\" in messages.columns and \"subject\" in messages.columns:\n",
    "      subj_map = dict(zip(messages[\"id\"].astype(str), messages[\"subject\"].astype(str)))\n",
    "      df[\"message_subject\"] = df[\"message_id_norm\"].map(subj_map)\n",
    "    else:\n",
    "      df[\"message_subject\"] = \"\"\n",
    "\n",
    "    # automation id from campaign\n",
    "    automation_id = None\n",
    "    if \"seriesid\" in campaigns.columns:\n",
    "      # en campañas de automatización, seriesid suele referenciar la automation\n",
    "      df[\"automation_id_norm\"] = df[\"campaign_id_norm\"].map(dict(zip(campaigns[\"id\"].astype(str), campaigns[\"seriesid\"].astype(str))))\n",
    "    else:\n",
    "      df[\"automation_id_norm\"] = \"\"\n",
    "\n",
    "    # fallback: parse links.automation\n",
    "    if \"links\" in campaigns.columns:\n",
    "      # links puede venir como dict o como string; intentamos parsearlo si parece JSON\n",
    "      def _get_link_automation(v: Any) -> str:\n",
    "        if isinstance(v, dict):\n",
    "          return extract_id_from_url(v.get(\"automation\", \"\"))\n",
    "        s = str(v)\n",
    "        if s.startswith(\"{\") and \"automation\" in s:\n",
    "          try:\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, dict):\n",
    "              return extract_id_from_url(obj.get(\"automation\", \"\"))\n",
    "          except Exception:\n",
    "            return \"\"\n",
    "        return \"\"\n",
    "      link_map = dict(zip(campaigns[\"id\"].astype(str), campaigns[\"links\"].apply(_get_link_automation)))\n",
    "      df.loc[df[\"automation_id_norm\"].isin([\"\", \"0\", \"None\", \"nan\"]), \"automation_id_norm\"] = df[\"campaign_id_norm\"].map(link_map)\n",
    "\n",
    "    # automation name\n",
    "    if \"id\" in automations.columns and \"name\" in automations.columns:\n",
    "      aut_name_map = dict(zip(automations[\"id\"].astype(str), automations[\"name\"].astype(str)))\n",
    "      df[\"automation_name\"] = df[\"automation_id_norm\"].map(aut_name_map)\n",
    "    else:\n",
    "      df[\"automation_name\"] = \"\"\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def enrich_contact_automations(\n",
    "  df_ca: pd.DataFrame,\n",
    "  dims: Dict[str, pd.DataFrame],\n",
    ") -> pd.DataFrame:\n",
    "  if df_ca is None or df_ca.empty:\n",
    "    return df_ca\n",
    "  df = df_ca.copy()\n",
    "  automations = dims.get(\"automations\", pd.DataFrame()).copy()\n",
    "  if \"id\" in automations.columns and \"name\" in automations.columns:\n",
    "    m = dict(zip(automations[\"id\"].astype(str), automations[\"name\"].astype(str)))\n",
    "    # campos típicos: automation o seriesid\n",
    "    if \"automation\" in df.columns:\n",
    "      df[\"automation_name\"] = df[\"automation\"].astype(str).map(m)\n",
    "    elif \"seriesid\" in df.columns:\n",
    "      df[\"automation_name\"] = df[\"seriesid\"].astype(str).map(m)\n",
    "    else:\n",
    "      df[\"automation_name\"] = \"\"\n",
    "  return df\n",
    "\n",
    "\n",
    "def enrich_automation_entry_counts(\n",
    "  df_aec: pd.DataFrame,\n",
    "  dims: Dict[str, pd.DataFrame],\n",
    ") -> pd.DataFrame:\n",
    "  if df_aec is None or df_aec.empty:\n",
    "    return df_aec\n",
    "  df = df_aec.copy()\n",
    "  automations = dims.get(\"automations\", pd.DataFrame()).copy()\n",
    "  if \"id\" in automations.columns and \"name\" in automations.columns:\n",
    "    m = dict(zip(automations[\"id\"].astype(str), automations[\"name\"].astype(str)))\n",
    "    # en aec el id es automation_id\n",
    "    if \"id\" in df.columns:\n",
    "      df[\"automation_name_dim\"] = df[\"id\"].astype(str).map(m)\n",
    "  return df\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MART: Chatter maestro (tabla legible por contacto)\n",
    "# -------------------------\n",
    "def _read_best_table(output_base: str, table_name: str) -> pd.DataFrame:\n",
    "  \"\"\"Lee master/latest si existe, si no master/raw. Si no existe, devuelve df vacío.\"\"\"\n",
    "  p_latest = os.path.join(output_base, \"master\", \"latest\", f\"{table_name}.csv\")\n",
    "  p_raw = os.path.join(output_base, \"master\", \"raw\", f\"{table_name}.csv\")\n",
    "  if os.path.exists(p_latest):\n",
    "    return pd.read_csv(p_latest, dtype=str)\n",
    "  if os.path.exists(p_raw):\n",
    "    return pd.read_csv(p_raw, dtype=str)\n",
    "  return pd.DataFrame()\n",
    "\n",
    "\n",
    "def _mk_name(first: str, last: str, fallback: str = \"\") -> str:\n",
    "  full = f\"{(first or '').strip()} {(last or '').strip()}\".strip()\n",
    "  return full if full else (fallback or \"\")\n",
    "\n",
    "\n",
    "def build_chatter_master(\n",
    "  *,\n",
    "  output_base: str,\n",
    "  run_id: str,\n",
    "  dims: Dict[str, pd.DataFrame],\n",
    "  contact_ids_in_run: List[str],\n",
    "  max_events_per_contact: int = 2000,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "  \"\"\"\n",
    "  Construye un FACT unificado de \"eventos\" (chatter) y una tabla DIGEST por contacto.\n",
    "\n",
    "  - chatter_master: 1 fila = 1 evento con columnas legibles (campaign_name, message_subject, tag_name...)\n",
    "  - contact_digest: 1 fila = 1 contacto con \"mensaje\" resumido (sin IA, solo plantilla)\n",
    "\n",
    "  No borra info: los RAW siguen siendo la fuente de verdad.\n",
    "  \"\"\"\n",
    "  # --- Dim maps\n",
    "  tags = dims.get(\"tags\", pd.DataFrame())\n",
    "  lists = dims.get(\"lists\", pd.DataFrame())\n",
    "  automations = dims.get(\"automations\", pd.DataFrame())\n",
    "  campaigns = dims.get(\"campaigns\", pd.DataFrame())\n",
    "  messages = dims.get(\"messages\", pd.DataFrame())\n",
    "  users = dims.get(\"users\", pd.DataFrame())\n",
    "  deal_groups = dims.get(\"dealGroups\", pd.DataFrame())\n",
    "  deal_stages = dims.get(\"dealStages\", pd.DataFrame())\n",
    "  accounts = dims.get(\"accounts\", pd.DataFrame())\n",
    "  scores = dims.get(\"scores\", pd.DataFrame())\n",
    "\n",
    "  tag_name_map = {}\n",
    "  if not tags.empty and \"id\" in tags.columns:\n",
    "    name_col = \"tag\" if \"tag\" in tags.columns else (\"name\" if \"name\" in tags.columns else \"\")\n",
    "    if name_col:\n",
    "      tag_name_map = dict(zip(tags[\"id\"].astype(str), tags[name_col].astype(str)))\n",
    "\n",
    "  list_name_map = {}\n",
    "  if not lists.empty and \"id\" in lists.columns and \"name\" in lists.columns:\n",
    "    list_name_map = dict(zip(lists[\"id\"].astype(str), lists[\"name\"].astype(str)))\n",
    "\n",
    "  aut_name_map = {}\n",
    "  if not automations.empty and \"id\" in automations.columns and \"name\" in automations.columns:\n",
    "    aut_name_map = dict(zip(automations[\"id\"].astype(str), automations[\"name\"].astype(str)))\n",
    "\n",
    "  camp_name_map = {}\n",
    "  camp_msg_map = {}\n",
    "  camp_aut_map = {}\n",
    "  if not campaigns.empty and \"id\" in campaigns.columns:\n",
    "    if \"name\" in campaigns.columns:\n",
    "      camp_name_map = dict(zip(campaigns[\"id\"].astype(str), campaigns[\"name\"].astype(str)))\n",
    "    if \"message_id\" in campaigns.columns:\n",
    "      camp_msg_map = dict(zip(campaigns[\"id\"].astype(str), campaigns[\"message_id\"].astype(str)))\n",
    "    if \"seriesid\" in campaigns.columns:\n",
    "      camp_aut_map = dict(zip(campaigns[\"id\"].astype(str), campaigns[\"seriesid\"].astype(str)))\n",
    "    # fallback links.automation\n",
    "    if \"links\" in campaigns.columns:\n",
    "      def _get_link_automation(v: Any) -> str:\n",
    "        if isinstance(v, dict):\n",
    "          return extract_id_from_url(v.get(\"automation\", \"\"))\n",
    "        s = str(v)\n",
    "        if s.startswith(\"{\") and \"automation\" in s:\n",
    "          try:\n",
    "            obj = json.loads(s)\n",
    "            if isinstance(obj, dict):\n",
    "              return extract_id_from_url(obj.get(\"automation\", \"\"))\n",
    "          except Exception:\n",
    "            return \"\"\n",
    "        return \"\"\n",
    "      link_aut_map = dict(zip(campaigns[\"id\"].astype(str), campaigns[\"links\"].apply(_get_link_automation)))\n",
    "      # solo completa donde no hay seriesid\n",
    "      for k, v in link_aut_map.items():\n",
    "        if k not in camp_aut_map or str(camp_aut_map.get(k, \"\")).strip() in (\"\", \"0\", \"None\", \"nan\"):\n",
    "          camp_aut_map[k] = v\n",
    "\n",
    "  msg_subject_map = {}\n",
    "  if not messages.empty and \"id\" in messages.columns:\n",
    "    subj_col = \"subject\" if \"subject\" in messages.columns else (\"title\" if \"title\" in messages.columns else \"\")\n",
    "    if subj_col:\n",
    "      msg_subject_map = dict(zip(messages[\"id\"].astype(str), messages[subj_col].astype(str)))\n",
    "\n",
    "  user_name_map = {}\n",
    "  if not users.empty and \"id\" in users.columns:\n",
    "    # nombre amigable\n",
    "    fn = users.get(\"firstName\") if \"firstName\" in users.columns else \"\"\n",
    "    ln = users.get(\"lastName\") if \"lastName\" in users.columns else \"\"\n",
    "    if isinstance(fn, pd.Series) and isinstance(ln, pd.Series):\n",
    "      tmp = users.copy()\n",
    "      tmp[\"user_name\"] = [\n",
    "        _mk_name(str(a) if a is not None else \"\", str(b) if b is not None else \"\", fallback=str(tmp.iloc[i].get(\"email\", \"\")))\n",
    "        for i, (a, b) in enumerate(zip(tmp[\"firstName\"], tmp[\"lastName\"]))\n",
    "      ]\n",
    "      user_name_map = dict(zip(tmp[\"id\"].astype(str), tmp[\"user_name\"].astype(str)))\n",
    "    elif \"email\" in users.columns:\n",
    "      user_name_map = dict(zip(users[\"id\"].astype(str), users[\"email\"].astype(str)))\n",
    "\n",
    "  stage_name_map = {}\n",
    "  if not deal_stages.empty and \"id\" in deal_stages.columns and \"title\" in deal_stages.columns:\n",
    "    stage_name_map = dict(zip(deal_stages[\"id\"].astype(str), deal_stages[\"title\"].astype(str)))\n",
    "\n",
    "  pipeline_name_map = {}\n",
    "  if not deal_groups.empty and \"id\" in deal_groups.columns and \"title\" in deal_groups.columns:\n",
    "    pipeline_name_map = dict(zip(deal_groups[\"id\"].astype(str), deal_groups[\"title\"].astype(str)))\n",
    "\n",
    "  account_name_map = {}\n",
    "  if not accounts.empty and \"id\" in accounts.columns:\n",
    "    nm = \"name\" if \"name\" in accounts.columns else (\"account\" if \"account\" in accounts.columns else \"\")\n",
    "    if nm:\n",
    "      account_name_map = dict(zip(accounts[\"id\"].astype(str), accounts[nm].astype(str)))\n",
    "\n",
    "  score_name_map = {}\n",
    "  if not scores.empty and \"id\" in scores.columns:\n",
    "    nm = \"name\" if \"name\" in scores.columns else (\"title\" if \"title\" in scores.columns else \"\")\n",
    "    if nm:\n",
    "      score_name_map = dict(zip(scores[\"id\"].astype(str), scores[nm].astype(str)))\n",
    "\n",
    "  # --- Base contact info\n",
    "  df_contacts = _read_best_table(output_base, \"contacts\")\n",
    "  if not df_contacts.empty and \"contact_id\" in df_contacts.columns:\n",
    "    df_contacts[\"contact_id\"] = df_contacts[\"contact_id\"].astype(str)\n",
    "  else:\n",
    "    df_contacts = pd.DataFrame(columns=[\"contact_id\", \"email\", \"firstName\", \"lastName\"])\n",
    "\n",
    "  # --- Read fact-ish tables\n",
    "  df_acts = _read_best_table(output_base, \"activities\")\n",
    "  df_email = _read_best_table(output_base, \"emailActivities_enriched\")\n",
    "  df_notes = _read_best_table(output_base, \"contactNotes\")\n",
    "  df_tasks = _read_best_table(output_base, \"contactTasks\")\n",
    "  df_logs = _read_best_table(output_base, \"contactLogs\")\n",
    "  df_track = _read_best_table(output_base, \"trackingLogs\")\n",
    "  df_bounce = _read_best_table(output_base, \"bounceLogs\")\n",
    "  df_geo = _read_best_table(output_base, \"geoIps\")\n",
    "  df_goals = _read_best_table(output_base, \"contactGoals\")\n",
    "  df_ca = _read_best_table(output_base, \"contactAutomations\")\n",
    "  df_deal_acts = _read_best_table(output_base, \"dealActivities\")\n",
    "  df_deal_notes = _read_best_table(output_base, \"dealNotes\")\n",
    "  df_deal_tasks = _read_best_table(output_base, \"dealTasks\")\n",
    "  df_deals = _read_best_table(output_base, \"deals\")\n",
    "\n",
    "  # --- Deal title map (para enriquecer eventos deal*)\n",
    "  deal_title_map = {}\n",
    "  deal_stage_map = {}\n",
    "  deal_pipeline_map = {}\n",
    "  if not df_deals.empty and \"id\" in df_deals.columns:\n",
    "    if \"title\" in df_deals.columns:\n",
    "      deal_title_map = dict(zip(df_deals[\"id\"].astype(str), df_deals[\"title\"].astype(str)))\n",
    "    if \"stage\" in df_deals.columns:\n",
    "      deal_stage_map = dict(zip(df_deals[\"id\"].astype(str), df_deals[\"stage\"].astype(str)))\n",
    "    if \"group\" in df_deals.columns:\n",
    "      deal_pipeline_map = dict(zip(df_deals[\"id\"].astype(str), df_deals[\"group\"].astype(str)))\n",
    "\n",
    "  # --- Helpers\n",
    "  def _pick_ts(row: pd.Series, candidates: List[str]) -> str:\n",
    "    for c in candidates:\n",
    "      if c in row and pd.notna(row[c]) and str(row[c]).strip():\n",
    "        return str(row[c]).strip()\n",
    "    return \"\"\n",
    "\n",
    "  def _as_event_rows(df_src: pd.DataFrame, source: str) -> List[Dict[str, Any]]:\n",
    "    if df_src is None or df_src.empty:\n",
    "      return []\n",
    "\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    # solo eventos de los contactos procesados en esta corrida (para el delta), pero master completo se arma con todos\n",
    "    df_use = df_src.copy()\n",
    "    if \"contact_id\" in df_use.columns:\n",
    "      df_use[\"contact_id\"] = df_use[\"contact_id\"].astype(str)\n",
    "    else:\n",
    "      df_use[\"contact_id\"] = \"\"\n",
    "\n",
    "    # límite por contacto para no explotar memoria (tomamos los eventos mas recientes por timestamp disponible)\n",
    "    ts_candidates = [\"tstamp\", \"udate\", \"cdate\", \"lastdate\", \"mdate\", \"duedate\", \"created_timestamp\", \"updated_timestamp\"]\n",
    "    ts_col = next((c for c in ts_candidates if c in df_use.columns), None)\n",
    "    if \"contact_id\" in df_use.columns and ts_col:\n",
    "      try:\n",
    "        df_use[\"_ts_sort\"] = pd.to_datetime(df_use[ts_col], errors=\"coerce\", utc=True)\n",
    "        df_use = df_use.sort_values(by=[\"contact_id\", \"_ts_sort\"], ascending=[True, False], kind=\"mergesort\")\n",
    "        df_use = df_use.groupby(\"contact_id\").head(max_events_per_contact).reset_index(drop=True)\n",
    "      finally:\n",
    "        if \"_ts_sort\" in df_use.columns:\n",
    "          df_use = df_use.drop(columns=[\"_ts_sort\"], errors=\"ignore\")\n",
    "\n",
    "    for _, r in df_use.iterrows():\n",
    "      cid = str(r.get(\"contact_id\", \"\")).strip()\n",
    "      rid = str(r.get(\"id\", \"\")).strip() or str(r.get(\"_row_hash\", \"\")).strip()\n",
    "\n",
    "      evt = {\n",
    "        \"event_id\": f\"{source}:{rid}\" if rid else f\"{source}:{hashlib.md5(str(dict(r)).encode('utf-8')).hexdigest()}\",\n",
    "        \"contact_id\": cid,\n",
    "        \"source\": source,\n",
    "        \"source_row_id\": rid,\n",
    "        \"event_ts\": \"\",\n",
    "        \"event_type\": \"\",\n",
    "        \"title\": \"\",\n",
    "        \"detail\": \"\",\n",
    "        \"campaign_id\": \"\",\n",
    "        \"campaign_name\": \"\",\n",
    "        \"message_id\": \"\",\n",
    "        \"message_subject\": \"\",\n",
    "        \"automation_id\": \"\",\n",
    "        \"automation_name\": \"\",\n",
    "        \"list_id\": \"\",\n",
    "        \"list_name\": \"\",\n",
    "        \"tag_id\": \"\",\n",
    "        \"tag_name\": \"\",\n",
    "        \"deal_id\": \"\",\n",
    "        \"deal_title\": \"\",\n",
    "        \"deal_stage_id\": \"\",\n",
    "        \"deal_stage_name\": \"\",\n",
    "        \"pipeline_id\": \"\",\n",
    "        \"pipeline_name\": \"\",\n",
    "        \"user_id\": \"\",\n",
    "        \"user_name\": \"\",\n",
    "        \"url\": \"\",\n",
    "      }\n",
    "\n",
    "      # ---- Source-specific mappings ----\n",
    "      if source == \"emailActivities_enriched\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"tstamp\", \"cdate\", \"udate\"])\n",
    "        evt[\"event_type\"] = str(r.get(\"type\", \"email\"))\n",
    "        evt[\"campaign_id\"] = str(r.get(\"campaignid\", r.get(\"campaign_id_norm\", \"\")))\n",
    "        evt[\"campaign_name\"] = str(r.get(\"campaign_name\", \"\")) or camp_name_map.get(evt[\"campaign_id\"], \"\")\n",
    "        evt[\"message_id\"] = str(r.get(\"message_id_norm\", r.get(\"messageid\", \"\")))\n",
    "        if not evt[\"message_id\"] and evt[\"campaign_id\"]:\n",
    "          evt[\"message_id\"] = camp_msg_map.get(evt[\"campaign_id\"], \"\")\n",
    "        evt[\"message_subject\"] = str(r.get(\"message_subject\", \"\")) or msg_subject_map.get(evt[\"message_id\"], \"\")\n",
    "        evt[\"automation_id\"] = str(r.get(\"automation_id_norm\", \"\")) or camp_aut_map.get(evt[\"campaign_id\"], \"\")\n",
    "        evt[\"automation_name\"] = str(r.get(\"automation_name\", \"\")) or aut_name_map.get(evt[\"automation_id\"], \"\")\n",
    "        evt[\"title\"] = f\"Email {evt['event_type']}: {evt['message_subject']}\".strip(\": \")\n",
    "        evt[\"detail\"] = f\"Campaign: {evt['campaign_name']} | Automation: {evt['automation_name']}\".strip()\n",
    "\n",
    "      elif source == \"activities\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"tstamp\", \"cdate\"])\n",
    "        ref_type = str(r.get(\"reference_type\", r.get(\"referenceType\", \"\")))\n",
    "        ref_action = str(r.get(\"reference_action\", r.get(\"referenceAction\", \"\")))\n",
    "        ref_id = str(r.get(\"reference_id\", r.get(\"referenceId\", \"\")))\n",
    "        evt[\"event_type\"] = f\"{ref_type}:{ref_action}\".strip(\":\")\n",
    "        # intentos de traducción\n",
    "        if ref_type.lower().startswith(\"campaign\"):\n",
    "          evt[\"campaign_id\"] = ref_id\n",
    "          evt[\"campaign_name\"] = camp_name_map.get(ref_id, \"\")\n",
    "          evt[\"title\"] = f\"Campaign {ref_action}: {evt['campaign_name']}\".strip(\": \")\n",
    "        elif ref_type.lower().startswith(\"message\"):\n",
    "          evt[\"message_id\"] = ref_id\n",
    "          evt[\"message_subject\"] = msg_subject_map.get(ref_id, \"\")\n",
    "          evt[\"title\"] = f\"Message {ref_action}: {evt['message_subject']}\".strip(\": \")\n",
    "        elif ref_type.lower().startswith(\"automation\"):\n",
    "          evt[\"automation_id\"] = ref_id\n",
    "          evt[\"automation_name\"] = aut_name_map.get(ref_id, \"\")\n",
    "          evt[\"title\"] = f\"Automation {ref_action}: {evt['automation_name']}\".strip(\": \")\n",
    "        else:\n",
    "          evt[\"title\"] = f\"Activity {evt['event_type']}\".strip()\n",
    "        evt[\"detail\"] = str(r.get(\"description\", r.get(\"text\", \"\")))\n",
    "        uid = str(r.get(\"user\", r.get(\"userid\", \"\")))\n",
    "        evt[\"user_id\"] = uid\n",
    "        evt[\"user_name\"] = user_name_map.get(uid, \"\")\n",
    "\n",
    "      elif source == \"contactNotes\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"cdate\", \"udate\"])\n",
    "        evt[\"event_type\"] = \"note\"\n",
    "        evt[\"title\"] = \"Nota\" \n",
    "        evt[\"detail\"] = str(r.get(\"note\", r.get(\"text\", \"\")))\n",
    "        uid = str(r.get(\"userid\", r.get(\"user\", \"\")))\n",
    "        evt[\"user_id\"] = uid\n",
    "        evt[\"user_name\"] = user_name_map.get(uid, \"\")\n",
    "\n",
    "      elif source == \"contactTasks\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"udate\", \"cdate\", \"duedate\"])\n",
    "        evt[\"event_type\"] = \"task\"\n",
    "        evt[\"title\"] = str(r.get(\"title\", \"Tarea\"))\n",
    "        evt[\"detail\"] = str(r.get(\"note\", r.get(\"description\", \"\")))\n",
    "        uid = str(r.get(\"userid\", r.get(\"user\", \"\")))\n",
    "        evt[\"user_id\"] = uid\n",
    "        evt[\"user_name\"] = user_name_map.get(uid, \"\")\n",
    "\n",
    "      elif source == \"contactLogs\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"tstamp\", \"cdate\"])\n",
    "        evt[\"event_type\"] = \"log\"\n",
    "        evt[\"title\"] = str(r.get(\"action\", \"Log\"))\n",
    "        evt[\"detail\"] = str(r.get(\"message\", r.get(\"description\", \"\")))\n",
    "\n",
    "      elif source == \"trackingLogs\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"tstamp\", \"cdate\"])\n",
    "        evt[\"event_type\"] = \"visit\"\n",
    "        evt[\"url\"] = str(r.get(\"url\", r.get(\"link\", \"\")))\n",
    "        evt[\"title\"] = f\"Visita: {evt['url']}\".strip()\n",
    "\n",
    "      elif source == \"bounceLogs\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"tstamp\", \"cdate\"])\n",
    "        evt[\"event_type\"] = \"bounce\"\n",
    "        evt[\"title\"] = \"Bounce\"\n",
    "        evt[\"detail\"] = str(r.get(\"reason\", r.get(\"message\", \"\")))\n",
    "\n",
    "      elif source == \"geoIps\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"tstamp\", \"cdate\"])\n",
    "        evt[\"event_type\"] = \"geo_ip\"\n",
    "        evt[\"title\"] = f\"IP: {str(r.get('ip', ''))}\".strip()\n",
    "        evt[\"detail\"] = str(r.get(\"country\", \"\"))\n",
    "\n",
    "      elif source == \"contactGoals\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"cdate\", \"tstamp\"])\n",
    "        evt[\"event_type\"] = \"goal\"\n",
    "        evt[\"title\"] = str(r.get(\"name\", r.get(\"goal\", \"Goal\")))\n",
    "\n",
    "      elif source == \"contactAutomations\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"lastdate\", \"adddate\", \"cdate\"])\n",
    "        evt[\"event_type\"] = \"automation_status\"\n",
    "        aid = str(r.get(\"automation\", r.get(\"seriesid\", \"\")))\n",
    "        evt[\"automation_id\"] = aid\n",
    "        evt[\"automation_name\"] = str(r.get(\"automation_name\", \"\")) or aut_name_map.get(aid, \"\")\n",
    "        evt[\"title\"] = f\"Automation: {evt['automation_name']}\"\n",
    "        evt[\"detail\"] = f\"status={str(r.get('status', ''))}\"\n",
    "\n",
    "      elif source == \"dealActivities\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"cdate\", \"tstamp\"])\n",
    "        evt[\"event_type\"] = \"deal_activity\"\n",
    "        did = str(r.get(\"deal_id\", r.get(\"d_id\", \"\")))\n",
    "        evt[\"deal_id\"] = did\n",
    "        evt[\"deal_title\"] = deal_title_map.get(did, \"\")\n",
    "        stid = str(r.get(\"d_stageid\", \"\")) or deal_stage_map.get(did, \"\")\n",
    "        evt[\"deal_stage_id\"] = stid\n",
    "        evt[\"deal_stage_name\"] = stage_name_map.get(stid, \"\")\n",
    "        gid = str(r.get(\"d_groupid\", \"\")) or deal_pipeline_map.get(did, \"\")\n",
    "        evt[\"pipeline_id\"] = gid\n",
    "        evt[\"pipeline_name\"] = pipeline_name_map.get(gid, \"\")\n",
    "        evt[\"title\"] = f\"Deal: {evt['deal_title']}\"\n",
    "        evt[\"detail\"] = f\"stage={evt['deal_stage_name']} action={str(r.get('dataAction',''))}\"\n",
    "        uid = str(r.get(\"userid\", \"\"))\n",
    "        evt[\"user_id\"] = uid\n",
    "        evt[\"user_name\"] = user_name_map.get(uid, \"\")\n",
    "\n",
    "      elif source == \"dealNotes\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"cdate\", \"udate\"])\n",
    "        evt[\"event_type\"] = \"deal_note\"\n",
    "        did = str(r.get(\"deal_id\", \"\"))\n",
    "        evt[\"deal_id\"] = did\n",
    "        evt[\"deal_title\"] = deal_title_map.get(did, \"\")\n",
    "        evt[\"title\"] = f\"Nota deal: {evt['deal_title']}\"\n",
    "        evt[\"detail\"] = str(r.get(\"note\", r.get(\"text\", \"\")))\n",
    "\n",
    "      elif source == \"dealTasks\":\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"udate\", \"cdate\", \"duedate\"])\n",
    "        evt[\"event_type\"] = \"deal_task\"\n",
    "        did = str(r.get(\"deal_id\", \"\"))\n",
    "        evt[\"deal_id\"] = did\n",
    "        evt[\"deal_title\"] = deal_title_map.get(did, \"\")\n",
    "        evt[\"title\"] = f\"Tarea deal: {evt['deal_title']}\"\n",
    "        evt[\"detail\"] = str(r.get(\"title\", \"\"))\n",
    "\n",
    "      else:\n",
    "        evt[\"event_ts\"] = _pick_ts(r, [\"tstamp\", \"cdate\", \"udate\"])\n",
    "        evt[\"event_type\"] = source\n",
    "        evt[\"title\"] = source\n",
    "\n",
    "      out.append(evt)\n",
    "    return out\n",
    "\n",
    "  # --- Build events\n",
    "  events: List[Dict[str, Any]] = []\n",
    "  events.extend(_as_event_rows(df_email, \"emailActivities_enriched\"))\n",
    "  events.extend(_as_event_rows(df_acts, \"activities\"))\n",
    "  events.extend(_as_event_rows(df_notes, \"contactNotes\"))\n",
    "  events.extend(_as_event_rows(df_tasks, \"contactTasks\"))\n",
    "  events.extend(_as_event_rows(df_logs, \"contactLogs\"))\n",
    "  events.extend(_as_event_rows(df_track, \"trackingLogs\"))\n",
    "  events.extend(_as_event_rows(df_bounce, \"bounceLogs\"))\n",
    "  events.extend(_as_event_rows(df_geo, \"geoIps\"))\n",
    "  events.extend(_as_event_rows(df_goals, \"contactGoals\"))\n",
    "  events.extend(_as_event_rows(df_ca, \"contactAutomations\"))\n",
    "  events.extend(_as_event_rows(df_deal_acts, \"dealActivities\"))\n",
    "  events.extend(_as_event_rows(df_deal_notes, \"dealNotes\"))\n",
    "  events.extend(_as_event_rows(df_deal_tasks, \"dealTasks\"))\n",
    "\n",
    "  df_events = pd.DataFrame(events)\n",
    "  if not df_events.empty:\n",
    "    df_events[\"contact_id\"] = df_events[\"contact_id\"].astype(str)\n",
    "    # merge contacto\n",
    "    if not df_contacts.empty:\n",
    "      keep_cols = [c for c in [\"contact_id\", \"email\", \"firstName\", \"lastName\", \"phone\"] if c in df_contacts.columns]\n",
    "      df_events = df_events.merge(df_contacts[keep_cols], on=\"contact_id\", how=\"left\")\n",
    "      if \"firstName\" in df_events.columns or \"lastName\" in df_events.columns:\n",
    "        df_events[\"contact_name\"] = [\n",
    "          _mk_name(str(a) if a is not None else \"\", str(b) if b is not None else \"\", fallback=str(df_events.iloc[i].get(\"email\", \"\")))\n",
    "          for i, (a, b) in enumerate(zip(df_events.get(\"firstName\", pd.Series([\"\"]*len(df_events))), df_events.get(\"lastName\", pd.Series([\"\"]*len(df_events)))))\n",
    "        ]\n",
    "    # --- Normaliza timestamps y orden cronologico (local)\n",
    "    if \"event_ts\" in df_events.columns:\n",
    "      # UTC naive para ordenar de forma consistente\n",
    "      df_events[\"event_time_utc\"] = pd.to_datetime(df_events[\"event_ts\"], errors=\"coerce\", utc=True).dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "      # Hora local para lectura humana (LOCAL_TZ)\n",
    "      df_events[\"event_time_local\"] = pd.to_datetime(df_events[\"event_ts\"], errors=\"coerce\", utc=True).dt.tz_convert(\"UTC\")\n",
    "      df_events[\"event_time_local_str\"] = df_events[\"event_time_local\"].dt.strftime(\"%Y-%m-%d %H:%M:%S %z\")\n",
    "    else:\n",
    "      df_events[\"event_time_utc\"] = pd.NaT\n",
    "      df_events[\"event_time_local\"] = pd.NaT\n",
    "      df_events[\"event_time_local_str\"] = \"\"\n",
    "\n",
    "    # Linea de chatter lista para leer (sin perder columnas base)\n",
    "    def _mk_chatter_line(row: pd.Series) -> str:\n",
    "      ts = str(row.get(\"event_time_local_str\", \"\")).strip()\n",
    "      title = str(row.get(\"title\", \"\")).strip()\n",
    "      detail = str(row.get(\"detail\", \"\")).strip()\n",
    "      url = str(row.get(\"url\", \"\")).strip()\n",
    "      who = str(row.get(\"user_name\", \"\")).strip()\n",
    "      who_txt = f\" ({who})\" if who else \"\"\n",
    "      parts = [p for p in [title + who_txt, detail, url] if p and p != 'nan']\n",
    "      body = \" — \".join(parts)\n",
    "      return f\"{ts} — {body}\".strip(\" —\") if ts else body\n",
    "\n",
    "    df_events[\"chatter_line\"] = df_events.apply(_mk_chatter_line, axis=1)\n",
    "\n",
    "    # Orden cronologico: de mas antiguo a mas reciente (dentro de cada contacto)\n",
    "    df_events = df_events.sort_values(by=[\"contact_id\", \"event_time_utc\", \"event_id\"], ascending=[True, True, True], kind=\"mergesort\", na_position=\"last\")\n",
    "\n",
    "  # --- Build digest (texto simple)\n",
    "  # latest relations\n",
    "  df_ct = _read_best_table(output_base, \"contactTags\")\n",
    "  if not df_ct.empty:\n",
    "    df_ct[\"tag_name\"] = df_ct.get(\"tag\", \"\").astype(str).map(tag_name_map) if \"tag\" in df_ct.columns else \"\"\n",
    "  df_cl = _read_best_table(output_base, \"contactLists\")\n",
    "  if not df_cl.empty:\n",
    "    df_cl[\"list_name\"] = df_cl.get(\"list\", \"\").astype(str).map(list_name_map) if \"list\" in df_cl.columns else \"\"\n",
    "\n",
    "  df_digest_rows: List[Dict[str, Any]] = []\n",
    "  contact_set = set(contact_ids_in_run)\n",
    "  # si incremental_only -> digest para esos contactos, si no -> para seed+new\n",
    "  digest_ids = sorted(contact_set) if contact_set else sorted(df_events[\"contact_id\"].unique().tolist()) if not df_events.empty else []\n",
    "\n",
    "  for cid in digest_ids:\n",
    "    info = df_contacts[df_contacts[\"contact_id\"].astype(str) == str(cid)] if (not df_contacts.empty and \"contact_id\" in df_contacts.columns) else pd.DataFrame()\n",
    "    email = str(info.iloc[0].get(\"email\", \"\")) if not info.empty else \"\"\n",
    "    name = _mk_name(str(info.iloc[0].get(\"firstName\", \"\")) if not info.empty else \"\", str(info.iloc[0].get(\"lastName\", \"\")) if not info.empty else \"\", fallback=email)\n",
    "\n",
    "    # tags y listas actuales\n",
    "    tags_list = []\n",
    "    if not df_ct.empty:\n",
    "      tmp = df_ct[df_ct.get(\"contact_id\", \"\").astype(str) == str(cid)]\n",
    "      # preferimos tag_name, si no existe, tag id\n",
    "      if \"tag_name\" in tmp.columns:\n",
    "        tags_list = [t for t in tmp[\"tag_name\"].astype(str).tolist() if t and t != 'nan']\n",
    "      elif \"tag\" in tmp.columns:\n",
    "        tags_list = [t for t in tmp[\"tag\"].astype(str).tolist() if t and t != 'nan']\n",
    "      tags_list = sorted(set(tags_list))\n",
    "\n",
    "    lists_list = []\n",
    "    if not df_cl.empty:\n",
    "      tmp = df_cl[df_cl.get(\"contact_id\", \"\").astype(str) == str(cid)]\n",
    "      if \"list_name\" in tmp.columns:\n",
    "        lists_list = [t for t in tmp[\"list_name\"].astype(str).tolist() if t and t != 'nan']\n",
    "      elif \"list\" in tmp.columns:\n",
    "        lists_list = [t for t in tmp[\"list\"].astype(str).tolist() if t and t != 'nan']\n",
    "      lists_list = sorted(set(lists_list))\n",
    "\n",
    "    # automatizaciones actuales\n",
    "    autos_list = []\n",
    "    if not df_ca.empty:\n",
    "      tmp = df_ca[df_ca.get(\"contact_id\", \"\").astype(str) == str(cid)]\n",
    "      if \"automation_name\" in tmp.columns:\n",
    "        autos_list = [t for t in tmp[\"automation_name\"].astype(str).tolist() if t and t != 'nan']\n",
    "      elif \"automation\" in tmp.columns:\n",
    "        autos_list = [aut_name_map.get(str(a), str(a)) for a in tmp[\"automation\"].astype(str).tolist()]\n",
    "      autos_list = sorted(set(autos_list))\n",
    "\n",
    "    # últimos eventos\n",
    "    lines: List[str] = []\n",
    "    if not df_events.empty:\n",
    "      ev = df_events[df_events[\"contact_id\"].astype(str) == str(cid)].tail(25)\n",
    "      for _, er in ev.iterrows():\n",
    "        ts = str(er.get(\"event_ts\", \"\"))\n",
    "        ttl = str(er.get(\"title\", \"\"))\n",
    "        det = str(er.get(\"detail\", \"\"))\n",
    "        lines.append(f\"- {ts} | {ttl}\" + (f\" | {det}\" if det else \"\"))\n",
    "\n",
    "        message_lines = [\n",
    "      f\"Contacto: {name} ({email})\".strip(),\n",
    "      f\"Tags: {', '.join(tags_list) if tags_list else '-'}\",\n",
    "      f\"Lists: {', '.join(lists_list) if lists_list else '-'}\",\n",
    "      f\"Automations: {', '.join(autos_list) if autos_list else '-'}\",\n",
    "      \"Últimos eventos:\",\n",
    "    ]\n",
    "    message_lines.extend(lines if lines else [\"- (sin eventos)\"])\n",
    "    message = \"\n",
    "\".join(message_lines)\n",
    "df_digest_rows.append({\n",
    "      \"contact_id\": str(cid),\n",
    "      \"contact_name\": name,\n",
    "      \"email\": email,\n",
    "      \"tags\": \", \".join(tags_list),\n",
    "      \"lists\": \", \".join(lists_list),\n",
    "      \"automations\": \", \".join(autos_list),\n",
    "      \"digest\": message,\n",
    "    })\n",
    "\n",
    "  df_digest = pd.DataFrame(df_digest_rows)\n",
    "  return df_events, df_digest\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Contacts table\n",
    "# -------------------------\n",
    "def run_contacts_table(seed_resolved: pd.DataFrame, new_contacts: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "  df_new = pd.DataFrame(new_contacts) if new_contacts else pd.DataFrame()\n",
    "  df_seed = seed_resolved.copy() if seed_resolved is not None else pd.DataFrame(columns=[\"contact_id\", \"email\"])\n",
    "  if not df_new.empty and \"id\" in df_new.columns:\n",
    "    df_new = df_new.rename(columns={\"id\": \"contact_id\"})\n",
    "  if \"contact_id\" not in df_new.columns:\n",
    "    df_new[\"contact_id\"] = \"\"\n",
    "  df_new[\"contact_id\"] = df_new[\"contact_id\"].astype(str)\n",
    "\n",
    "  # merge básico para tener email del seed si el endpoint no lo trae\n",
    "  if not df_seed.empty:\n",
    "    df_seed[\"contact_id\"] = df_seed[\"contact_id\"].astype(str)\n",
    "    df_out = pd.merge(df_new, df_seed[[\"contact_id\", \"email\"]], on=\"contact_id\", how=\"left\", suffixes=(\"\", \"_seed\"))\n",
    "    if \"email\" in df_out.columns and \"email_seed\" in df_out.columns:\n",
    "      df_out[\"email\"] = df_out[\"email\"].fillna(\"\") # type: ignore\n",
    "      df_out.loc[df_out[\"email\"].astype(str).str.strip().eq(\"\"), \"email\"] = df_out[\"email_seed\"]\n",
    "      df_out = df_out.drop(columns=[\"email_seed\"])\n",
    "    return df_out\n",
    "  return df_new\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Orchestration\n",
    "# -------------------------\n",
    "def run_pipeline(\n",
    "  *,\n",
    "  base_url: str,\n",
    "  api_token: str,\n",
    "  output_base: str,\n",
    "  seed_csv: Optional[str] = None,\n",
    "  incremental_only: bool = False,\n",
    "  use_email_fallback: bool = True,\n",
    "  sample_size: Optional[int] = None,\n",
    "  refresh_dims_days: int = 7,\n",
    ") -> None:\n",
    "  ensure_dir(output_base)\n",
    "\n",
    "  cfg = ACConfig(base_url=base_url, api_token=api_token)\n",
    "  client = ACClient(cfg)\n",
    "\n",
    "  run_id = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "  ensure_dir(os.path.join(output_base, \"runs\", run_id, \"delta\"))\n",
    "\n",
    "  # Estado incremental (solo para nuevos contactos)\n",
    "  state_path = os.path.join(output_base, \"state.json\")\n",
    "  state = load_json(state_path, {\"max_contact_id\": 0, \"last_run_utc\": None})\n",
    "  max_from_state = safe_int(state.get(\"max_contact_id\", 0))\n",
    "\n",
    "  # Seed\n",
    "  seed_resolved = pd.DataFrame(columns=[\"contact_id\", \"email\"])\n",
    "  seed_ids: List[str] = []\n",
    "  max_from_seed = 0\n",
    "  if seed_csv:\n",
    "    seed_resolved = resolve_seed_contact_ids(client, seed_csv, use_email_fallback=use_email_fallback)\n",
    "    seed_ids = seed_resolved[\"contact_id\"].astype(str).tolist() if not seed_resolved.empty else []\n",
    "    max_from_seed = max((safe_int(x) for x in seed_ids), default=0)\n",
    "\n",
    "  # Nuevos contactos\n",
    "  start_from = max(max_from_state, max_from_seed)\n",
    "  new_contacts = client.get_contacts_after_id(start_from)\n",
    "  new_ids = [str(c.get(\"id\", \"\")).strip() for c in (new_contacts or []) if str(c.get(\"id\", \"\")).strip()]\n",
    "\n",
    "  # Universo\n",
    "  if incremental_only:\n",
    "    contact_ids = new_ids\n",
    "  else:\n",
    "    contact_ids = sorted(set(seed_ids + new_ids), key=lambda x: safe_int(x))\n",
    "  contact_ids = [c for c in contact_ids if c]\n",
    "\n",
    "  # Sample (debug)\n",
    "  if sample_size and sample_size > 0 and len(contact_ids) > sample_size:\n",
    "    contact_ids = contact_ids[:sample_size]\n",
    "\n",
    "  print(f\"[Run] run_id={run_id}\")\n",
    "  print(f\"[Incremental] start_from={start_from} | nuevos={len(new_ids)} | total_a_procesar={len(contact_ids)}\")\n",
    "\n",
    "  # Dims (cache)\n",
    "  dims = load_all_dims(client, output_base, refresh_days=refresh_dims_days)\n",
    "\n",
    "  # Contacts table\n",
    "  df_contacts = run_contacts_table(seed_resolved, new_contacts)\n",
    "  if not df_contacts.empty and \"contact_id\" in df_contacts.columns:\n",
    "    persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"contacts\", df_new=df_contacts, key_cols_latest=[\"contact_id\"], updated_at_col=\"udate\" if \"udate\" in df_contacts.columns else None)\n",
    "\n",
    "  if not contact_ids:\n",
    "    # actualiza estado aunque no haya nuevos (para registrar corrida)\n",
    "    state[\"last_run_utc\"] = utc_now_iso()\n",
    "    save_json(state_path, state)\n",
    "    print(\"[Done] No hay contactos para procesar.\")\n",
    "    return\n",
    "\n",
    "  # Chatter\n",
    "  df_acts = run_activities(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"activities\", df_new=df_acts, key_cols_latest=[\"id\"] if \"id\" in df_acts.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"tstamp\" if \"tstamp\" in df_acts.columns else None)\n",
    "\n",
    "  df_email = run_email_activities(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"emailActivities\", df_new=df_email, key_cols_latest=[\"id\"] if \"id\" in df_email.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"tstamp\" if \"tstamp\" in df_email.columns else None)\n",
    "\n",
    "  df_email_en = enrich_email_activities(df_email, dims)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"emailActivities_enriched\", df_new=df_email_en, key_cols_latest=[\"id\"] if \"id\" in df_email_en.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"tstamp\" if \"tstamp\" in df_email_en.columns else None)\n",
    "\n",
    "  df_notes = run_contact_notes(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"contactNotes\", df_new=df_notes, key_cols_latest=[\"id\"] if \"id\" in df_notes.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"cdate\" if \"cdate\" in df_notes.columns else None)\n",
    "\n",
    "  df_cl = run_contact_lists(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"contactLists\", df_new=df_cl, key_cols_latest=[\"id\"] if \"id\" in df_cl.columns else [\"contact_id\", \"list\"], updated_at_col=\"udate\" if \"udate\" in df_cl.columns else None)\n",
    "\n",
    "  df_ct = run_contact_tags(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"contactTags\", df_new=df_ct, key_cols_latest=[\"id\"] if \"id\" in df_ct.columns else [\"contact_id\", \"tag\"], updated_at_col=\"cdate\" if \"cdate\" in df_ct.columns else None)\n",
    "\n",
    "  df_logs = run_contact_logs(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"contactLogs\", df_new=df_logs, key_cols_latest=[\"id\"] if \"id\" in df_logs.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"tstamp\" if \"tstamp\" in df_logs.columns else None)\n",
    "\n",
    "  df_track = run_tracking_logs(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"trackingLogs\", df_new=df_track, key_cols_latest=[\"id\"] if \"id\" in df_track.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"tstamp\" if \"tstamp\" in df_track.columns else None)\n",
    "\n",
    "  df_bounce = run_bounce_logs(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"bounceLogs\", df_new=df_bounce, key_cols_latest=[\"id\"] if \"id\" in df_bounce.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"tstamp\" if \"tstamp\" in df_bounce.columns else None)\n",
    "\n",
    "  df_geo = run_geo_ips(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"geoIps\", df_new=df_geo, key_cols_latest=[\"id\"] if \"id\" in df_geo.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"tstamp\" if \"tstamp\" in df_geo.columns else None)\n",
    "\n",
    "  df_goals = run_contact_goals(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"contactGoals\", df_new=df_goals, key_cols_latest=[\"id\"] if \"id\" in df_goals.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"cdate\" if \"cdate\" in df_goals.columns else None)\n",
    "\n",
    "  df_cdata = run_contact_data(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"contactData\", df_new=df_cdata, key_cols_latest=[\"id\"] if \"id\" in df_cdata.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"updated_timestamp\" if \"updated_timestamp\" in df_cdata.columns else None)\n",
    "\n",
    "  df_scores = run_score_values(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"scoreValues\", df_new=df_scores, key_cols_latest=[\"id\"] if \"id\" in df_scores.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"tstamp\" if \"tstamp\" in df_scores.columns else None)\n",
    "\n",
    "  df_accts = run_account_contacts(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"accountContacts\", df_new=df_accts, key_cols_latest=[\"id\"] if \"id\" in df_accts.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"cdate\" if \"cdate\" in df_accts.columns else None)\n",
    "\n",
    "  df_ctasks = run_contact_tasks(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"contactTasks\", df_new=df_ctasks, key_cols_latest=[\"id\"] if \"id\" in df_ctasks.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"udate\" if \"udate\" in df_ctasks.columns else None)\n",
    "\n",
    "  # Automations (NEW)\n",
    "  df_ca = run_contact_automations(client, contact_ids)\n",
    "  df_ca_en = enrich_contact_automations(df_ca, dims)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"contactAutomations\", df_new=df_ca_en, key_cols_latest=[\"id\"] if \"id\" in df_ca_en.columns else [\"contact_id\", \"automation\"], updated_at_col=\"lastdate\" if \"lastdate\" in df_ca_en.columns else None)\n",
    "\n",
    "  df_aec = run_automation_entry_counts(client, contact_ids)\n",
    "  df_aec_en = enrich_automation_entry_counts(df_aec, dims)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"automationEntryCounts\", df_new=df_aec_en, key_cols_latest=[\"contact_id\", \"id\"], updated_at_col=None)\n",
    "\n",
    "  # Deals bundle\n",
    "  df_deals, df_deal_notes, df_deal_tasks, df_deal_acts = run_deals_bundle(client, contact_ids)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"deals\", df_new=df_deals, key_cols_latest=[\"id\"] if \"id\" in df_deals.columns else [\"contact_id\", \"_row_hash\"], updated_at_col=\"mdate\" if \"mdate\" in df_deals.columns else None)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"dealNotes\", df_new=df_deal_notes, key_cols_latest=[\"id\"] if \"id\" in df_deal_notes.columns else [\"deal_id\", \"_row_hash\"], updated_at_col=\"cdate\" if \"cdate\" in df_deal_notes.columns else None)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"dealTasks\", df_new=df_deal_tasks, key_cols_latest=[\"id\"] if \"id\" in df_deal_tasks.columns else [\"deal_id\", \"_row_hash\"], updated_at_col=\"udate\" if \"udate\" in df_deal_tasks.columns else None)\n",
    "  persist_table_dual(output_base=output_base, run_id=run_id, table_name=\"dealActivities\", df_new=df_deal_acts, key_cols_latest=[\"id\"] if \"id\" in df_deal_acts.columns else [\"deal_id\", \"_row_hash\"], updated_at_col=\"cdate\" if \"cdate\" in df_deal_acts.columns else None)\n",
    "\n",
    "  # MART: archivo maestro legible (chatter unificado + digest por contacto)\n",
    "  try:\n",
    "    df_chatter, df_digest = build_chatter_master(\n",
    "      output_base=output_base,\n",
    "      run_id=run_id,\n",
    "      dims=dims,\n",
    "      contact_ids_in_run=contact_ids,\n",
    "      max_events_per_contact=500,\n",
    "    )\n",
    "    mart_dir = os.path.join(output_base, \"master\", \"mart\")\n",
    "    ensure_dir(mart_dir)\n",
    "    if df_chatter is not None and not df_chatter.empty:\n",
    "      write_csv_utf8sig(os.path.join(mart_dir, \"chatter_master.csv\"), df_chatter)\n",
    "      # delta del run\n",
    "      write_csv_utf8sig(os.path.join(output_base, \"runs\", run_id, \"delta\", \"chatter_master.csv\"), df_chatter[df_chatter[\"contact_id\"].astype(str).isin(set(contact_ids))])\n",
    "    if df_digest is not None and not df_digest.empty:\n",
    "      write_csv_utf8sig(os.path.join(mart_dir, \"contact_digest.csv\"), df_digest)\n",
    "      write_csv_utf8sig(os.path.join(output_base, \"runs\", run_id, \"delta\", \"contact_digest.csv\"), df_digest)\n",
    "      # (opcional) 1 TXT por contacto\n",
    "      txt_dir = os.path.join(mart_dir, \"contact_digest_txt\")\n",
    "      ensure_dir(txt_dir)\n",
    "      for _, r in df_digest.iterrows():\n",
    "        cid = str(r.get(\"contact_id\", \"\"))\n",
    "        msg = str(r.get(\"digest\", \"\"))\n",
    "        if cid and msg:\n",
    "          with open(os.path.join(txt_dir, f\"contact_{cid}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg)\n",
    "  except Exception as e:\n",
    "    print(f\"[Warn] No se pudo construir chatter_master/digest: {e}\")\n",
    "\n",
    "  # Update state (max contact id visto)\n",
    "  new_max = max([safe_int(x) for x in (seed_ids + new_ids)] or [max_from_state])\n",
    "  state[\"max_contact_id\"] = max(new_max, max_from_state)\n",
    "  state[\"last_run_utc\"] = utc_now_iso()\n",
    "  save_json(state_path, state)\n",
    "\n",
    "  print(\"[Done] Export terminado.\")\n",
    "  print(f\"Salida: {output_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIG ====\n",
    "\n",
    "import os\n",
    "\n",
    "# Recomendado: usar variables de entorno\n",
    "# Windows PowerShell:\n",
    "#   $env:AC_BASE_URL=\"https://YOURACCOUNT.api-us1.com/api/3\"\n",
    "#   $env:AC_API_TOKEN=\"TU_TOKEN\"\n",
    "\n",
    "BASE_URL = os.getenv('AC_BASE_URL', 'https://YOURACCOUNT.api-us1.com/api/3')\n",
    "API_TOKEN = os.getenv('AC_API_TOKEN', '')\n",
    "\n",
    "# Carpeta de salida\n",
    "OUTPUT_BASE = r'ac_export'\n",
    "\n",
    "# CSV semilla (opcional). Si no tienes, deja ''\n",
    "SEED_CSV = r''\n",
    "\n",
    "# Solo nuevos (1) o semilla+nuevos (0)\n",
    "INCREMENTAL_ONLY = True\n",
    "\n",
    "# Si el seed no trae contact_id, intenta resolver por email (más lento)\n",
    "USE_EMAIL_FALLBACK = True\n",
    "\n",
    "# 0 = sin sample\n",
    "SAMPLE_SIZE = 0\n",
    "\n",
    "# Refrescar dims cada N días\n",
    "REFRESH_DIMS_DAYS = 7\n",
    "\n",
    "# Validaciones mínimas\n",
    "if not API_TOKEN:\n",
    "    print('⚠️ Falta AC_API_TOKEN en variables de entorno (o asígnalo a API_TOKEN).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== RUN ====\n",
    "\n",
    "import os\n",
    "\n",
    "run_pipeline(\n",
    "    base_url=BASE_URL,\n",
    "    api_token=API_TOKEN,\n",
    "    output_base=OUTPUT_BASE,\n",
    "    seed_csv=SEED_CSV if SEED_CSV else None,\n",
    "    incremental_only=INCREMENTAL_ONLY,\n",
    "    use_email_fallback=USE_EMAIL_FALLBACK,\n",
    "    sample_size=SAMPLE_SIZE if SAMPLE_SIZE > 0 else None,\n",
    "    refresh_dims_days=REFRESH_DIMS_DAYS,\n",
    ")\n",
    "\n",
    "print('Listo ✅')\n",
    "print('Output:', os.path.abspath(OUTPUT_BASE))\n",
    "print('Chatter master:', os.path.join(os.path.abspath(OUTPUT_BASE), 'master', 'mart', 'chatter_master.csv'))\n",
    "print('Digest:', os.path.join(os.path.abspath(OUTPUT_BASE), 'master', 'mart', 'contact_digest.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71271fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== QUICK PEEK (opcional) ====\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ch_path = os.path.join(OUTPUT_BASE, 'master', 'mart', 'chatter_master.csv')\n",
    "dg_path = os.path.join(OUTPUT_BASE, 'master', 'mart', 'contact_digest.csv')\n",
    "\n",
    "if os.path.exists(ch_path):\n",
    "    df = pd.read_csv(ch_path, dtype=str)\n",
    "    display(df.head(20))\n",
    "else:\n",
    "    print('No existe chatter_master.csv todavía:', ch_path)\n",
    "\n",
    "if os.path.exists(dg_path):\n",
    "    dg = pd.read_csv(dg_path, dtype=str)\n",
    "    display(dg.head(20))\n",
    "else:\n",
    "    print('No existe contact_digest.csv todavía:', dg_path)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}